{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112023d6-9f2c-4129-88c9-e18329874263",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "GroqError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGroqError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableBranch,RunnablePassthrough,RunnableLambda\n\u001b[0;32m     23\u001b[0m dotenv\u001b[38;5;241m.\u001b[39mload_dotenv()\n\u001b[1;32m---> 24\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatGroq(\n\u001b[0;32m     25\u001b[0m         groq_api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROQ_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     26\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3-8b-8192\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[0;32m     30\u001b[0m urls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://gradspace.org/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://gradspace.org/courses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\load\\serializable.py:110\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_groq\\chat_models.py:404\u001b[0m, in \u001b[0;36mChatGroq.validate_environment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    402\u001b[0m sync_specific: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m groq\u001b[38;5;241m.\u001b[39mGroq(\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msync_specific\n\u001b[0;32m    406\u001b[0m     )\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n\u001b[0;32m    408\u001b[0m     async_specific: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_async_client}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\groq\\_client.py:89\u001b[0m, in \u001b[0;36mGroq.__init__\u001b[1;34m(self, api_key, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m     87\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROQ_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GroqError(\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m     )\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mGroqError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import os\n",
    "import dotenv\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables import RunnableBranch,RunnablePassthrough,RunnableLambda\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "llm = ChatGroq(\n",
    "        groq_api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "        model_name=\"llama3-8b-8192\"\n",
    ")\n",
    "\n",
    "# load data\n",
    "urls = [\"https://gradspace.org/\",\"https://gradspace.org/courses\"]\n",
    "loaders = UnstructuredURLLoader(urls = urls)\n",
    "data = loaders.load()\n",
    "\n",
    "\n",
    "# splitting data\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "all_splits  = text_splitter.split_documents(data)\n",
    "\n",
    "# load Instructor Embeddings\n",
    "embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-base\")\n",
    "\n",
    "# create vector database\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(all_splits, embeddings)\n",
    "\n",
    "# define the retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1748ccc-e607-4eec-ac03-75ba544b7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query transformation\n",
    "\n",
    "def create_query_chain():\n",
    "    query_transform_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # query_transformation_chain = query_transform_prompt | llm\n",
    "    \n",
    "    \n",
    "    # query transforming retriever\n",
    "    query_transforming_retriever_chain = RunnableBranch(\n",
    "        (\n",
    "            lambda x: len(x.get(\"messages\", [])) == 1,\n",
    "            # If only one message, then we just pass that message's content to retriever\n",
    "            (lambda x: x[\"messages\"][-1].content) | retriever,\n",
    "        ),\n",
    "        # If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever\n",
    "        query_transform_prompt | llm | StrOutputParser() | retriever,\n",
    "    ).with_config(run_name=\"chat_retriever_chain\")\n",
    "    0\n",
    "    # conversational retrieval chain\n",
    "    SYSTEM_TEMPLATE = \"\"\"\n",
    "    Answer the user's questions based on the below context. \n",
    "    If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n",
    "    \n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    "    \n",
    "    question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                SYSTEM_TEMPLATE,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    document_chain = create_stuff_documents_chain(llm, question_answering_prompt)\n",
    "    \n",
    "    conversational_retrieval_chain = RunnablePassthrough.assign(context=query_transforming_retriever_chain,) | document_chain\n",
    "\n",
    "    return conversational_retrieval_chain\n",
    "\n",
    "# conversational_retrieval_chain = create_query_chain()\n",
    "\n",
    "# conversational_retrieval_chain.invoke(\n",
    "#     {\n",
    "#         \"messages\": [\n",
    "#             HumanMessage(content=\"backend development\"),\n",
    "#         ]\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a814d-6895-4c1f-949d-cfbfeacf169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "students = {}\n",
    "\n",
    "# define a tool to collect student information\n",
    "@tool\n",
    "def register(name, age, email):\n",
    "    \"\"\"Register a student with name and email\"\"\"\n",
    "    global students\n",
    "    students[email] = [name,age,email]\n",
    "    return AIMessage(content=\"You have been registered.\")\n",
    "\n",
    "@tool\n",
    "def students_display():\n",
    "    \"\"\"display all enrollments\"\"\"\n",
    "    global students\n",
    "    disp =''\n",
    "    for email,infos in students.items():\n",
    "        disp +='name: '+infos[0]+' \\t age: '+str(infos[1]) + ' \\t email: '+infos[2] \n",
    "        disp +='\\n'\n",
    "    return AIMessage(content=disp)\n",
    "    \n",
    "def create_register_chain():\n",
    "    # define a tool chain\n",
    "    register_tool_llm = llm.bind_tools([register])\n",
    "    register_tool_chain = register_tool_llm | (lambda x: x.tool_calls[0][\"args\"]) | register\n",
    "\n",
    "    # define a data preprocessing chain before tool chain\n",
    "\n",
    "    register_data_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"\"\"\n",
    "                Given the above conversation, extract  user name, user age and user email, if information is unkown, leave it empty, do not make up data.\n",
    "                \n",
    "                << FORMATTING >>\n",
    "                Return a JSON object formatted without anything else:\n",
    "                ```json\n",
    "                {{\n",
    "                    \"name\": user name\n",
    "                    \"age\": user age\n",
    "                    \"email\": user email\n",
    "                }}\n",
    "                ```\n",
    "                \"\"\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    register_data_chain = register_data_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    \n",
    "    # define compound chain of data chain and tool chain\n",
    "    register_function_chain = register_data_chain | register_tool_chain\n",
    "    \n",
    "    \n",
    "    # define a chain ask the user for missing information\n",
    "\n",
    "    register_ask_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"\"\"\n",
    "                Given the above conversation, check user name, user age and user email are fully provided.\n",
    "                remind user to provide the missing information.\n",
    "                Don't make up data.\n",
    "                \"\"\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    register_ask_chain = register_ask_prompt | llm\n",
    "\n",
    "\n",
    "    # define a display chain\n",
    "    register_display_llm = llm.bind_tools([students_display])\n",
    "    register_display_chain = (lambda x: x[\"messages\"][-1].content) | register_display_llm | (lambda x: x.tool_calls[-1][\"args\"]) | students_display\n",
    "    \n",
    "    # define a router chain\n",
    "    register_router_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"\"\"\n",
    "                Given the above conversation,\n",
    "                If user name, age and email are all provided, respond with \"register\"\n",
    "                elif user want see all students details, respond with \"display\".\n",
    "                else respond with \"moreinfo\".\n",
    "\n",
    "                Do not respond with other words.\n",
    "                \"\"\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    register_router_chain = register_router_prompt | llm | StrOutputParser()\n",
    "    \n",
    "  \n",
    "    def validate(info):\n",
    "        if \"register\" in info[\"choice\"].lower():\n",
    "            return register_function_chain\n",
    "        elif \"display\" in info[\"choice\"].lower():\n",
    "            return register_display_chain\n",
    "        else:\n",
    "            return register_ask_chain\n",
    "    \n",
    "    # define a router chain, implement the tool or require more information\n",
    "    register_chain =  {\"choice\": register_router_chain, \"messages\": lambda x: x[\"messages\"]} | RunnableLambda(validate) | StrOutputParser()\n",
    "\n",
    "    return register_chain\n",
    "\n",
    "# chain = create_register_chain()\n",
    "# chain.invoke(\n",
    "#     {\n",
    "#         \"messages\": [\n",
    "#             HumanMessage(content=\"my name is steven\"),\n",
    "#         ]\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8804b-6862-45b0-bd92-6ff47ffd8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrollment = {}\n",
    "\n",
    "# define a tool for enrollment\n",
    "@tool\n",
    "def enroll(name, course):\n",
    "    \"\"\"enroll a student with name and course\"\"\"\n",
    "    global enrollment\n",
    "    if course in enrollment:\n",
    "        enrollment[course].append(name)\n",
    "    else:\n",
    "        enrollment[course]=[name]\n",
    "    return AIMessage(content=\"Successful Enrollment.\")\n",
    "\n",
    "@tool\n",
    "def enroll_display():\n",
    "    \"\"\"display all enrollments\"\"\"\n",
    "    global enrollment\n",
    "    info=''\n",
    "    for course,students in enrollment.items():\n",
    "        info +=course+': '\n",
    "        for student in students:\n",
    "            info +=student+' '\n",
    "        info+='\\n'\n",
    "    return AIMessage(content=info)\n",
    "    \n",
    "# define a enroll chain\n",
    "def create_enroll_chain():\n",
    " \n",
    "    enroll_display_llm = llm.bind_tools([enroll_display])\n",
    "    enroll_display_chain = (lambda x: x[\"messages\"][-1].content) | enroll_display_llm | (lambda x: x.tool_calls[-1][\"args\"]) | enroll_display\n",
    "    \n",
    "    # define a tool chain for \n",
    "    enroll_tool_llm = llm.bind_tools([enroll])\n",
    "    enroll_tool_chain = enroll_tool_llm | (lambda x: x.tool_calls[-1][\"args\"]) | enroll\n",
    "\n",
    "    # define a data preprocessing chain before tool chain\n",
    "\n",
    "    enroll_data_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"\"\"\n",
    "                Given the above conversation,\n",
    "                Extract  user name, course name. if information is unkown, leave it empty, do not make up data.\n",
    "                \n",
    "                << FORMATTING >>\n",
    "                Return a JSON object formatted without anything else:\n",
    "                ```json\n",
    "                {{\n",
    "                    \"name\": user name\n",
    "                    \"course\": course name\n",
    "                }}\n",
    "                ```\n",
    "                \"\"\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    enroll_data_chain = enroll_data_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    \n",
    "    # define compound chain of data chain and tool chain\n",
    "    enroll_function_chain = enroll_data_chain | enroll_tool_chain\n",
    "    \n",
    "    \n",
    "    # define a chain ask the user for missing information\n",
    "    enroll_ask_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"\"\"\n",
    "                Given the above conversation,\n",
    "                check whether user name, course name are fully provided.\n",
    "                remind user to provide the missing information.\n",
    "                Don't make up data.\n",
    "                \"\"\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    enroll_ask_chain =  enroll_ask_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    \n",
    "    # define a router chain\n",
    "    enroll_router_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"\"\"\n",
    "                Given the above conversation,\n",
    "                If user name and course name are provided, respond with \"register\"\n",
    "                if user want to show all enrollments or courses, respond with \"display\".\n",
    "                otherwise respond with \"moreinfo\".\n",
    "                Do not respond with other words.\n",
    "                \"\"\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    enroll_router_chain = enroll_router_prompt | llm | StrOutputParser()\n",
    "    \n",
    "       \n",
    "    # define a router chain, implement the tool or require more information\n",
    "    def enroll_router(info):\n",
    "        if \"register\" in info[\"choice\"].lower():\n",
    "            return enroll_function_chain\n",
    "        elif \"display\" in info[\"choice\"].lower():\n",
    "            return enroll_display_chain\n",
    "        else:\n",
    "            return enroll_ask_chain\n",
    "\n",
    "    enroll_chain =  {\"choice\": enroll_router_chain, \"messages\": lambda x: x[\"messages\"]} | RunnableLambda(enroll_router) | StrOutputParser()\n",
    "    \n",
    "    return enroll_chain\n",
    "\n",
    "# chain = create_enroll_chain()\n",
    "\n",
    "# chain.invoke(\n",
    "#     {\n",
    "#         \"messages\": [\n",
    "#             HumanMessage(content=\"my name is Steven\"),\n",
    "#         ]\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf332b6e-67b2-46fd-bbde-dce99c1bcfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full chain router\n",
    "def create_full_chain():\n",
    "    enroll_chain = create_enroll_chain()\n",
    "    register_chain = create_register_chain()\n",
    "    conversational_retrieval_chain = create_query_chain()\n",
    "\n",
    "    # create a router chain\n",
    "    classify_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"\"\"\n",
    "                Given the above conversation,\n",
    "                if user ask about courses or teachers, respond with \"query\".\n",
    "                If user discussed about student details, respond with `user detail`\n",
    "                If user discussed about enrollment, respond with `course enrollment`\n",
    "                otherwise respond with \"query\".\n",
    "                Do not respond with other words.\n",
    "                \"\"\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    classify_chain = classify_prompt | llm | StrOutputParser()\n",
    "    \n",
    "\n",
    "    # #create a default chain\n",
    "    # default_prompt = ChatPromptTemplate.from_messages(\n",
    "    #     [\n",
    "    #         MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    #         (\n",
    "    #             \"user\",\n",
    "    #             \"\"\"\n",
    "    #             Acting as a receptionist of an school, help students with all kinds of questions.\n",
    "    #             Don't make up data.\n",
    "    #             \"\"\",\n",
    "    #         ),\n",
    "    #     ]\n",
    "    # )\n",
    "    # default_chain = default_prompt | llm | StrOutputParser()\n",
    "    \n",
    "\n",
    "    # define a router chain to select between register and query\n",
    "    def route(info):\n",
    "        if \"user detail\" in info[\"topic\"].lower():\n",
    "            return register_chain\n",
    "        elif \"course enrollment\" in info[\"topic\"].lower():\n",
    "            return enroll_chain\n",
    "        else:\n",
    "            return conversational_retrieval_chain\n",
    "            \n",
    "    full_chain =  {\"topic\": classify_chain, \"messages\": lambda x: x[\"messages\"]} | RunnableLambda(route) | StrOutputParser()\n",
    "    \n",
    "    return full_chain\n",
    "\n",
    "# full_chain = create_full_chain()\n",
    "# full_chain.invoke(\n",
    "#     {\n",
    "#         \"messages\": [\n",
    "#             # HumanMessage(content=\"is there any courses about Backend Development?\"),\n",
    "#             # HumanMessage(content=\"my name is steven, my age is 20, my email is fdsf@gmail.com\"),\n",
    "#             # HumanMessage(content=\"my name is steven, I want to enroll Math course \"),\n",
    "#             # HumanMessage(content=\"display all enrollments\"),\n",
    "#         ]\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6456e-177b-4b85-8212-f36a41e32ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "\n",
    "# create a chatbot class\n",
    "class cbfs():\n",
    "    pn.extension()\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        self.answer = \"\"\n",
    "        self.panels = []\n",
    "        self.full_chain = create_full_chain()\n",
    "    \n",
    "\n",
    "    #  load the conversational retrieval Chain, and store the results\n",
    "    def convchain(self, query):\n",
    "        \"\"\"\n",
    "        query\n",
    "        \"\"\"\n",
    "        if not query:\n",
    "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
    "            \n",
    "        self.messages.append(HumanMessage(content=query))\n",
    "        self.answer = self.full_chain.invoke({\"messages\": self.messages})\n",
    "        self.messages.append(AIMessage(content=self.answer))\n",
    "        \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600))\n",
    "        ])\n",
    "        inp.value = ''  \n",
    "        return pn.WidgetBox(*self.panels,scroll=True)\n",
    "\n",
    "    \n",
    "    def clr_history(self,count=0):\n",
    "        self.messages = []\n",
    "        return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4cb537-eed0-4b8f-9249-bcfd168d70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the chatbot\n",
    "cb = cbfs() \n",
    "\n",
    "# create the panel\n",
    "\n",
    "\n",
    "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning') \n",
    "button_clearhistory.on_click(cb.clr_history) \n",
    "inp = pn.widgets.TextInput( placeholder='Enter text hereâ€¦') \n",
    "\n",
    "\n",
    "# bind the panel button with conversational retrieval Chain \n",
    "conversation = pn.bind(cb.convchain, inp) \n",
    "\n",
    "# define the interface panel\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.Row(button_clearhistory),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# ChatBot')),\n",
    "    pn.Tabs(('Conversation', tab1))\n",
    ")\n",
    "dashboard.servable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10c847-57df-4022-88bd-61467ded71ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
